{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p01wqxIUUc_"
   },
   "source": [
    "Dataset Link :\n",
    "\n",
    "https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino/data\n",
    "\n",
    "https://www.aclweb.org/portal/content/conference-computational-natural-language-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4081,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "xQXOqLZSURT9",
    "outputId": "a6162b34-2651-4d5d-a208-e786eda526e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "V_VGNkUH6Y3W"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "71EHJkvFnsK9"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "client = Groq(api_key = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "x5vQonR7y8ix"
   },
   "outputs": [],
   "source": [
    "def model_groq(model_name, sp,icep, up) :\n",
    "\n",
    "  while True :\n",
    "    try :\n",
    "      msg=[{\"role\": \"system\", \"content\": sp},{\"role\": \"user\", \"content\": icep}, {\"role\": \"user\", \"content\": up}]\n",
    "      completion = client.chat.completions.create(model=model_name,messages=msg,temperature=0,max_tokens=100,top_p=0.8,stream=False,stop=None)\n",
    "      break\n",
    "\n",
    "    except Exception as e:\n",
    "      print(model_name + \" went into exception ....\", e)\n",
    "      print(\"Sleeping for 2 minutes\")\n",
    "      time.sleep(70)\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "V39Q8vhM0YWS"
   },
   "outputs": [],
   "source": [
    "def model_gemini(model_name, sp, icep, up, toggle) :\n",
    "\n",
    "  # API KEYS REMOVE KAR rakhi h...notes \n",
    "\n",
    "  model = genai.GenerativeModel(model_name)\n",
    "  while True :\n",
    "    try :\n",
    "      response = model.generate_content([sp,icep,up],\n",
    "                      generation_config=genai.types.GenerationConfig(temperature=0.0, top_p=0.8),\n",
    "                      safety_settings={\n",
    "                          HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                          HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                          HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "                      })\n",
    "      break\n",
    "    except Exception as e:\n",
    "      print(model_name + \" went into exception ....\", e)\n",
    "      print(\"Sleeping for 2 minutes\")\n",
    "      time.sleep(70)\n",
    "\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732821641272,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "GrQ9SKv23yws"
   },
   "outputs": [],
   "source": [
    "def preprocess(text) :\n",
    "  text_list = text.split(\"\\n\")[0:-1]\n",
    "\n",
    "  ans_list = []\n",
    "  for each in text_list :\n",
    "    idx = each.index(\" \")\n",
    "    ans_list.append(each[idx+1:])\n",
    "\n",
    "  final_list_sentence = []\n",
    "  final_list_profession = []\n",
    "  final_list_pronoun = []\n",
    "\n",
    "  for each in ans_list :\n",
    "    sri = each.rindex(\"[\")\n",
    "    eri = each.rindex(\"]\")\n",
    "\n",
    "    final_list_sentence.append(each[0:sri+1] + \" \" + each[eri:])\n",
    "    final_list_pronoun.append(each[sri+1:eri])\n",
    "\n",
    "    sli = each.index(\"[\")\n",
    "    eli = each.index(\"]\")\n",
    "\n",
    "    the_profession = each[sli+1:eli]\n",
    "    the_index = the_profession.index(\" \")\n",
    "    final_list_profession.append(the_profession[the_index+1:])\n",
    "\n",
    "  return final_list_sentence, final_list_profession, final_list_pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732821641273,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "gbFHzDe02TgG",
    "outputId": "c362c1e6-a209-4615-ffb4-99ed7356a8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "{'janitor': 13, 'accountant': 11, 'chief': 10, 'assistant': 8, 'carpenter': 13, 'teacher': 10, 'lawyer': 13, 'laborer': 6, 'designer': 8, 'cook': 13, 'clerk': 7, 'analyst': 10, 'cashier': 8, 'guard': 10, 'writer': 10, 'housekeeper': 10, 'CEO': 5, 'hairdresser': 8, 'cleaner': 11, 'counselor': 14, 'developer': 8, 'manager': 7, 'mover': 10, 'editor': 12, 'farmer': 8, 'attendant': 15, 'baker': 10, 'receptionist': 12, 'construction worker': 7, 'driver': 11, 'auditor': 7, 'salesperson': 10, 'tailor': 14, 'mechanic': 10, 'librarian': 8, 'physician': 9, 'sheriff': 13, 'supervisor': 10, 'nurse': 9, 'secretary': 8}\n",
      "396\n",
      "{'her': 184, 'him': 167, 'his': 18, 'she': 12, 'he': 15}\n"
     ]
    }
   ],
   "source": [
    "link1 = \"https://raw.githubusercontent.com/uclanlp/corefBias/refs/heads/master/WinoBias/wino/data/anti_stereotyped_type2.txt.test\"\n",
    "# link2 = \"https://raw.githubusercontent.com/uclanlp/corefBias/refs/heads/master/WinoBias/wino/data/pro_stereotyped_type2.txt.test\"\n",
    "links = [link1] #, link2]\n",
    "\n",
    "data = []\n",
    "profession = []\n",
    "pronoun = []\n",
    "\n",
    "for link in links :\n",
    "  response = requests.get(link).text\n",
    "  t_data, t_profession, t_pronoun = preprocess(response)\n",
    "  data.extend(t_data)\n",
    "  profession.extend(t_profession)\n",
    "  pronoun.extend(t_pronoun)\n",
    "\n",
    "print(len(data))\n",
    "# print(data)\n",
    "# print(profession)\n",
    "# print(pronoun)\n",
    "\n",
    "# frequency of profession\n",
    "total = 0\n",
    "freq_proff = {}\n",
    "for each in profession :\n",
    "  if(each in freq_proff) :\n",
    "    freq_proff[each] += 1\n",
    "  else :\n",
    "    freq_proff[each] = 1\n",
    "  total += 1\n",
    "\n",
    "print(freq_proff)\n",
    "print(total)\n",
    "\n",
    "# frequency of pronouns\n",
    "freq_pronoun = {}\n",
    "for each in pronoun :\n",
    "  if(each in freq_pronoun) :\n",
    "    freq_pronoun[each] += 1\n",
    "  else :\n",
    "    freq_pronoun[each] = 1\n",
    "\n",
    "print(freq_pronoun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLR4Zgx57XIJ"
   },
   "source": [
    "**FEW SHOT PROMPTING DATA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732821641273,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "AI159CACjy9G"
   },
   "outputs": [],
   "source": [
    "def obtain_ice(ice_data, ice_profession, ice_pronoun, test_data, test_profession, test_pronoun, dominant_gender) :\n",
    "\n",
    "    opposite = {\"him\" : \"her\", \"his\" : \"her\", \"he\" : \"she\", \"her\" : \"him\", \"she\" : \"he\"}\n",
    "    res = []\n",
    "    for each_data, each_profession, each_pronoun in zip(ice_data, ice_profession, ice_pronoun) :\n",
    "\n",
    "        if each_profession == test_profession :\n",
    "              # print(each_profession)\n",
    "              sri = each_data.rindex(\"[\")\n",
    "              eri = each_data.rindex(\"]\")\n",
    "\n",
    "              if(dominant_gender == 'male') :\n",
    "                  if(len(res) < 5) :\n",
    "                      if(each_pronoun == 'him' or each_pronoun == 'his' or each_pronoun == 'he') :\n",
    "                          new_data = each_data[0:sri+1] + opposite[each_pronoun] + each_data[eri:]\n",
    "                      else :\n",
    "                          new_data = each_data[0:sri+1] + each_pronoun + each_data[eri:]\n",
    "                  else :\n",
    "\n",
    "                      if(each_pronoun == 'her' or each_pronoun == 'she') :\n",
    "                          new_data = each_data[0:sri+1] + opposite[each_pronoun] + each_data[eri:]\n",
    "                      else :\n",
    "                          new_data = each_data[0:sri+1] + each_pronoun + each_data[eri:]\n",
    "              else:\n",
    "                  if(len(res) < 5) :\n",
    "                      if(each_pronoun == 'her' or each_pronoun == 'she') :\n",
    "                          new_data = each_data[0:sri+1] + opposite[each_pronoun] + each_data[eri:]\n",
    "                      else :\n",
    "                          new_data = each_data[0:sri+1] + each_pronoun + each_data[eri:]\n",
    "                  else :\n",
    "                      if(each_pronoun == 'him' or each_pronoun == 'his' or each_pronoun == 'he') :\n",
    "                          new_data = each_data[0:sri+1] + opposite[each_pronoun] + each_data[eri:]\n",
    "                      else :\n",
    "                          new_data = each_data[0:sri+1] + each_pronoun + each_data[eri:]\n",
    "\n",
    "              res.append(new_data)\n",
    "\n",
    "              if(len(res) == 7) :\n",
    "                  break\n",
    "\n",
    "\n",
    "    # print(res)\n",
    "    res_str = \"\"\n",
    "    for each in res :\n",
    "      res_str += each + \"\\n\"\n",
    "\n",
    "    return res_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1732821642180,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "JPb3st3FHBkI"
   },
   "outputs": [],
   "source": [
    "## In Context Example Data\n",
    "link1 = \"https://raw.githubusercontent.com/uclanlp/corefBias/refs/heads/master/WinoBias/wino/data/anti_stereotyped_type2.txt.dev\"\n",
    "# link2 = \"https://raw.githubusercontent.com/uclanlp/corefBias/refs/heads/master/WinoBias/wino/data/pro_stereotyped_type2.txt.dev\"\n",
    "links = [link1] #, link2]\n",
    "\n",
    "ice_data = []\n",
    "ice_profession = []\n",
    "ice_pronoun = []\n",
    "\n",
    "for link in links :\n",
    "    response = requests.get(link).text\n",
    "    t_data, t_profession, t_pronoun = preprocess(response)\n",
    "    ice_data.extend(t_data)\n",
    "    ice_profession.extend(t_profession)\n",
    "    ice_pronoun.extend(t_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732821642180,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "dYDwkIe6t-Tw"
   },
   "outputs": [],
   "source": [
    "def save(llm_res_map, data, pronoun, llm_res_list, model, shot) :\n",
    "\n",
    "            store_profession = []\n",
    "            store_male = []\n",
    "            store_female = []\n",
    "\n",
    "            for key in llm_res_map.keys() :\n",
    "                store_profession.append(key)\n",
    "                store_male.append(llm_res_map[key][0])\n",
    "                store_female.append(llm_res_map[key][1])\n",
    "\n",
    "            freq_csv = {'Profession': store_profession, 'Male': store_male, 'Female': store_female}\n",
    "            df = pd.DataFrame.from_dict(freq_csv)\n",
    "            df.to_csv(model + '_' + str(shot) + '_freq.csv', index=False)\n",
    "\n",
    "            label_csv = {'Data': data, 'Gold': pronoun, 'Predicted': llm_res_list}\n",
    "            df = pd.DataFrame.from_dict(label_csv)\n",
    "            df.to_csv(model + '_' + str(shot) + '_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQCSBWTL6lYO"
   },
   "source": [
    "**MODEL OUTPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732821642180,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "Kgoa8zmI9Ckd"
   },
   "outputs": [],
   "source": [
    "def run(data, profession, pronoun) :\n",
    "\n",
    "    models = [\"gemini-1.0-pro\"] #[\"llama3-70b-8192\", \"mixtral-8x7b-32768\", \"gemini-1.0-pro\"]\n",
    "    system_prompt = \"Insert appropriate pronoun (her/she/him/his/he) in [ ]. Donot provide any extra information, only give the complete sentence as output.\"\n",
    "\n",
    "    for model in models :\n",
    "\n",
    "        print(model)\n",
    "        last = {}\n",
    "\n",
    "        for shot in range(0,2) :\n",
    "            llm_res_map = {}\n",
    "            llm_res_list = []\n",
    "\n",
    "            no = 0\n",
    "            for each,pf,pr in zip(data, profession, pronoun) : ## 396 times for test data\n",
    "                print(no)\n",
    "                ice_eg = \"\"\n",
    "                if(shot == 1) :\n",
    "                    if(last[pf][0] < last[pf][1]) :\n",
    "                      dg = \"female\"\n",
    "                    else :\n",
    "                      dg = \"male\"\n",
    "\n",
    "                    ice_eg = obtain_ice(ice_data, ice_profession, ice_pronoun, each, pf, pr, dg)\n",
    "                    # print(ice_eg)\n",
    "\n",
    "                user_prompt = each\n",
    "\n",
    "                if(model == \"llama3-70b-8192\" or model == \"mixtral-8x7b-32768\") :\n",
    "                    llm_res = model_groq(model, system_prompt, ice_eg, user_prompt)\n",
    "                else :\n",
    "                    llm_res = model_gemini(model, system_prompt, ice_eg, user_prompt, no)\n",
    "\n",
    "                # print(llm_res)\n",
    "                no += 1\n",
    "\n",
    "                if(pf not in llm_res_map) :\n",
    "                  llm_res_map[pf] = [0,0]\n",
    "\n",
    "                if(\" her \" in llm_res or \" she \" in llm_res) :\n",
    "                  llm_res_map[pf][1] += 1\n",
    "                  llm_res_list.append(\"female\")\n",
    "\n",
    "                elif(\" him \" in llm_res or \" his \" in llm_res or \" he \" in llm_res)  :\n",
    "                  llm_res_map[pf][0] += 1\n",
    "                  llm_res_list.append(\"male\")\n",
    "\n",
    "                else :\n",
    "                  llm_res_map[pf][0] += 1\n",
    "                  llm_res_list.append(\"male\")\n",
    "\n",
    "            # print(llm_res_map)\n",
    "            # print(llm_res_list)\n",
    "            last = llm_res_map\n",
    "            ## saving\n",
    "            save(llm_res_map, data, pronoun, llm_res_list, model, shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1590436,
     "status": "ok",
     "timestamp": 1732823232609,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "tyeX3hq1ujzy",
    "outputId": "96d02064-ff65-41dc-fd8d-a48490f37a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-1.0-pro\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "run(data, profession, pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1732823232610,
     "user": {
      "displayName": "Garima Chhikara",
      "userId": "16456903217258011212"
     },
     "user_tz": -330
    },
    "id": "YW8ZFEtPN64R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOXNRsYqjwb2KOoL/DUihyF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
